{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/connordouglas10/DS-Tech-2024spring/blob/main/Module2_Supervised/Supervised_segmentation.ipynb\" target=\"_parent\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" /> </a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o27tIlWJbf-f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#If opening in colab run this cell\n",
        "!git clone https://github.com/connordouglas10/DS-Tech-2024spring.git\n",
        "%cd DS-Tech-2024spring/Module2_Supervised/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W2pCzDSbf-i"
      },
      "source": [
        "# Supervised segmentation\n",
        "\n",
        "\n",
        "Spring 2024 - Instructors: Foster Provost and Connor Douglas\n",
        "\n",
        "Teaching Assistant: Connor Douglas\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8u8aHhWbf-l"
      },
      "source": [
        "## Some general imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuSW0WuBbf-l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(style='ticks', palette='Set2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWoPLvYXbf-m"
      },
      "source": [
        "## Predicting who will survive the Titanic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_JA1erfbf-n"
      },
      "source": [
        "This time we will use a clasic introductory dataset that contains demographic and traveling information for the Titanic passengers. The goal is to predict the survival of these passengers. We will only keep a few variables of interest and transform all of them to numeric variables. We will also drop some outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jux0umIEbf-p"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "path = \"./data/titanic.csv\"\n",
        "# We will drop missing values, which helps us to discuss the things we want to discuss in class;\n",
        "#   but note: that's not necessarily what you want to do.  Make sure to think through the decision-making context.\n",
        "#   Maybe things aren't missing at random, and you'll introduce some bias that will bite you later.\n",
        "#   OR, maybe missingness will be predictive! (Be careful of leakage in this case.)\n",
        "df = pd.read_csv(path)[[\"survived\", \"pclass\", \"sex\", \"age\", \"fare\"]].dropna()\n",
        "# Transform \"sex\" column to a numeric variable\n",
        "df[\"female\"] = (df.sex == \"female\").astype(int)\n",
        "df = df.drop(\"sex\", axis=\"columns\")\n",
        "# Drop outliers. This is to help the visualization in the next examples.\n",
        "df = df[df.fare < 400]\n",
        "# Take a look at the data\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pJtymElbf-q"
      },
      "source": [
        "We'd like to use data about the passengers to predict whether they will survive. Let's start by taking a look at how well some of the variables \"split\" the data according to our target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA77aiDG5Ord"
      },
      "outputs": [],
      "source": [
        "#What is the *base rate* of survival?\n",
        "df.hist(\"survived\", bins=2)\n",
        "plt.title(\"survived\")\n",
        "plt.show\n",
        "df[\"survived\"].value_counts()/df[\"survived\"].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6TR4xmWbf-q"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x=\"survived\", y=\"fare\", width=0.4, data=df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fkQ7HXGbf-s"
      },
      "source": [
        "Above we see boxplots that show the fare distribution grouped by our target variable (survival). The left boxplot corresponds to people that died and the right one to people that survived. Let's look at an alternative plot of the distribution of fare according to survival:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYt-cTYzbf-v"
      },
      "outputs": [],
      "source": [
        "for r in range(2):\n",
        "    hist = df[df.survived == r].hist('fare')\n",
        "    plt.title(\"survived =\" + str(r))\n",
        "    plt.ylim(0,470)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP3n8AURbf-w"
      },
      "source": [
        "On might conclude from this that people that paid less are less likely to survive. We could use this to predict that people that paid more than 50 will survive. How effective is this threshold? Let's quantify it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4hWZ0NAbf-x"
      },
      "source": [
        "***\n",
        "\n",
        "\n",
        "**Entropy** ($H$) and **information gain** ($IG$) are useful tools for measuring the effectiveness of a split on the values of one variable for giving information on the value of another variable. Entropy measures how random data is, information gain is a measure of the reduction in randomness after performing a split.\n",
        "\n",
        "<table style=\"border: 0px\">\n",
        "<tr style=\"border: 0px\">\n",
        "<td style=\"border: 0px\"><img src=\"https://github.com/pearl-yu/foster_2022fall/blob/2022-master/Module2_Supervised/images/dsfb_0304.png?raw=1\" height=80% width=80%>\n",
        "Figure 3-4. Splitting the \"write-off\" sample into two segments, based on splitting the Balance attribute (account balance) at 50K.</td>\n",
        "<td style=\"border: 0px; width: 30px\"></td>\n",
        "<td style=\"border: 0px\"><img src=\"https://github.com/pearl-yu/foster_2022fall/blob/2022-master/Module2_Supervised/images/dsfb_0305.png?raw=1\" height=120% width=120%>\n",
        "Figure 3-5. A classification tree split on the three-values Residence attribute.</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Given the data, it is fairly straight forward to calculate both of these quantities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ9Nt8jgbf-x"
      },
      "source": [
        "##### Functions to get the entropy and IG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYNk3OSgbf-x"
      },
      "outputs": [],
      "source": [
        "def entropy(target_column):\n",
        "    \"\"\"\n",
        "        computes -sum_i p_i * log_2 (p_i) for each i\n",
        "    \"\"\"\n",
        "    # get the counts of each target value\n",
        "    target_counts = target_column.value_counts().astype(float).values\n",
        "    total = target_column.count()\n",
        "    # compute probas\n",
        "    probas = target_counts/total\n",
        "    # p_i * log_2 (p_i)\n",
        "    entropy_components = probas * np.log2(probas)\n",
        "    # return negative sum\n",
        "    return - entropy_components.sum()\n",
        "\n",
        "def information_gain(df, info_column, target_column, threshold):\n",
        "    \"\"\"\n",
        "        computes H(target) - H(target | info > thresh) - H(target | info <= thresh)\n",
        "    \"\"\"\n",
        "    # split data\n",
        "    data_above_thresh = df[df[info_column] > threshold]\n",
        "    data_below_thresh = df[df[info_column] <= threshold]\n",
        "    # get entropy\n",
        "    H = entropy(df[target_column])\n",
        "    entropy_above = entropy(data_above_thresh[target_column])\n",
        "    entropy_below = entropy(data_below_thresh[target_column])\n",
        "    # compute weighted average\n",
        "    ct_above = data_above_thresh.shape[0]\n",
        "    ct_below = data_below_thresh.shape[0]\n",
        "    tot = float(df.shape[0])\n",
        "    return H - entropy_above*ct_above/tot - entropy_below*ct_below/tot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_p9qO_ibf-y"
      },
      "source": [
        "Now that we have a way of calculating $H$ and $IG$, let's test our prior hunch, that using 50 as a split on fare allows us to people who will survive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjOG65Q3bf-z"
      },
      "outputs": [],
      "source": [
        "threshold = 50\n",
        "prior_entropy = entropy(df[\"survived\"])\n",
        "IG = information_gain(df, \"fare\", \"survived\", threshold)\n",
        "print (\"IG of %.4f using a threshold of %.2f given a prior entropy of %.4f\" % (IG, threshold, prior_entropy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw8QsElmbf-0"
      },
      "source": [
        "How good was our guess of 50? Let's loop through all possible splits on fare and see what is the best!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_a0XcGtbf-0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def best_threshold(df, info_column, target_column, criteria=information_gain):\n",
        "    maximum_ig = 0\n",
        "    maximum_threshold = 0\n",
        "\n",
        "    for thresh in df[info_column].unique():\n",
        "        IG = criteria(df, info_column, target_column, thresh)\n",
        "        if IG > maximum_ig:\n",
        "            maximum_ig = IG\n",
        "            maximum_threshold = thresh\n",
        "\n",
        "    return (maximum_threshold, maximum_ig)\n",
        "\n",
        "maximum_threshold, maximum_ig = best_threshold(df, \"fare\", \"survived\")\n",
        "\n",
        "print (\"the maximum IG we can achieve splitting on %s is %.4f using a thresh of %.2f\" % ('fare', maximum_ig, maximum_threshold))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QINaTo2bf-1"
      },
      "source": [
        "Other observed features may also give us (strong) clues about survival."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDc-A3Vtbf--"
      },
      "outputs": [],
      "source": [
        "# Names of different columns\n",
        "categorical_cols = [\"pclass\", \"female\"]\n",
        "continuous_cols = [\"age\", \"fare\"]\n",
        "target_col = \"survived\"\n",
        "predictor_cols = categorical_cols + continuous_cols\n",
        "\n",
        "# This is to plot everything in a 2x2 space\n",
        "rows, cols = 2, 2\n",
        "fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(7*cols, 7*rows))\n",
        "axs = axs.flatten()\n",
        "posn = 0\n",
        "\n",
        "# Plot continous features\n",
        "for col in continuous_cols:\n",
        "    sns.boxplot(x=target_col, y=col, data=df, ax=axs[posn])\n",
        "    axs[posn].set_ylabel(col)\n",
        "    axs[posn].set_title(\"\")\n",
        "    posn += 1\n",
        "\n",
        "# Plot categorical features\n",
        "for col in categorical_cols:\n",
        "    df.groupby(col)[target_col].mean().plot(kind=\"bar\", ax=axs[posn])\n",
        "    axs[posn].set_ylabel(\"Fraction that survived\")\n",
        "    posn += 1\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRA10IcZbf-_"
      },
      "source": [
        "So, then ... what feature gives the most effective split?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmi72qrobf-_"
      },
      "outputs": [],
      "source": [
        "def best_split(df, info_columns, target_column, criteria=information_gain):\n",
        "    maximum_ig = 0\n",
        "    maximum_threshold = 0\n",
        "    maximum_column = \"\"\n",
        "\n",
        "    for info_column in info_columns:\n",
        "        thresh, ig = best_threshold(df, info_column, target_column, criteria)\n",
        "\n",
        "        if ig > maximum_ig:\n",
        "            maximum_ig = ig\n",
        "            maximum_threshold = thresh\n",
        "            maximum_column = info_column\n",
        "\n",
        "    return maximum_column, maximum_threshold, maximum_ig\n",
        "\n",
        "max_col, max_threshold, max_ig = best_split(df, predictor_cols, \"survived\")\n",
        "\n",
        "print (\"The best column to split on is %s giving us a IG of %.4f using a thresh of %.2f\" % (max_col, max_ig, max_threshold))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad_cAWI0bf_A"
      },
      "source": [
        "### The Classification Tree: Recursive Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owzBAy1sbf_B"
      },
      "source": [
        "Of course, splitting the data one time sometimes isn't enough to make accurate predictions. However, we can continue to split the data recursively (\"recursive partitioning\"), building a tree-structured model that may give better results.\n",
        "\n",
        "Tree-structured models are one of the most popular and powerful sorts of machine learning algorithm.  They include classification or class-probability estimation trees (aka \"decision trees\"), regression trees, and many more complex models built by combining tree-structured models, such as random forests and gradient boosted models (which usually combine trees).\n",
        "\n",
        "What are some other ways you might consider splitting the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjXBomvRbf_C"
      },
      "outputs": [],
      "source": [
        "import matplotlib.patches as mpatches\n",
        "\n",
        "cmap = {1: 'blue', 0: 'red'}\n",
        "df.plot(kind=\"scatter\", x=\"fare\", y=\"age\", c=[cmap[c] for c in df[target_col]])\n",
        "plt.legend(handles=[mpatches.Patch(color=cmap[k], label=k) for k in cmap], loc=1, title=\"Target\", frameon=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5WZt4Y7bf_C"
      },
      "source": [
        "Rather than build a classification tree from scratch (think if you could now do this!) let's use sklearn's implementation which includes some additional functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76pCWNhebf_C"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Define the model (tree)\n",
        "# ---> This is really both defining the model AND the ML algorithm to learn (train) it!\n",
        "# choose tree max depth (there are better ways to limit complexity, but this is very understandable)\n",
        "maxi_depth = 3\n",
        "our_decision_tree = DecisionTreeClassifier(max_depth=maxi_depth, criterion=\"entropy\")   # Look at those 2 arguments !!!\n",
        "# Tell the model what is the \"training\" data and then train it\n",
        "our_decision_tree.fit(df[predictor_cols], df[\"survived\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-agHi2Obf_D"
      },
      "source": [
        "We now have a classification tree, let's visualize the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jfqWaIJbf_D"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "def visualize_tree(decision_tree, feature_names, class_names, directory=\"./images\", name=\"tree\",proportion=True):\n",
        "    # Export our decision tree to graphviz format\n",
        "    dot_name = \"%s/%s.dot\" % (directory, name)\n",
        "    dot_file = export_graphviz(decision_tree, out_file=dot_name,\n",
        "                               feature_names=feature_names, class_names=class_names,proportion=proportion)\n",
        "    # Call graphviz to make an image file from our decision tree\n",
        "    image_name = \"%s/%s.png\" % (directory, name)\n",
        "    os.system(\"dot -T png %s -o %s\" % (dot_name, image_name))\n",
        "    # Return the .png image so we can see it\n",
        "    return Image(filename=image_name)\n",
        "\n",
        "visualize_tree(our_decision_tree, predictor_cols, [\"dies\", \"survives\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ4jASscbf_E"
      },
      "source": [
        "Let's look at `\"age\"` and `\"fare\"`, including the **DECISION SURFACE!!**\n",
        "\n",
        "More details for this graph: [sklearn decision surface](https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html#sphx-glr-auto-examples-tree-plot-iris-dtc-py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "19kI_nxXbf_F",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def Decision_Surface(data, col1, col2, target, model, probabilities=False):\n",
        "    # Get bounds\n",
        "    x_min, x_max = data[col1].min(), data[col1].max()\n",
        "    y_min, y_max = data[col2].min(), data[col2].max()\n",
        "    # Create a mesh\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.5), np.arange(y_min, y_max,0.5))\n",
        "    meshed_data = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()])\n",
        "    # Get predictions for the mesh\n",
        "    tdf = data[[col1, col2]]\n",
        "    model.fit(tdf, target)\n",
        "    if probabilities:\n",
        "        Z = model.predict_proba(meshed_data)[:, 1].reshape(xx.shape)\n",
        "    else:\n",
        "        Z = model.predict(meshed_data).reshape(xx.shape)\n",
        "    # Chart details\n",
        "    plt.figure(figsize=[12,7])\n",
        "    plt.title(\"Decision surface\")\n",
        "    plt.xlabel(col1)\n",
        "    plt.ylabel(col2)\n",
        "    if probabilities:\n",
        "        # Color-scale on the contour (surface = separator)\n",
        "        cs = plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm_r, alpha=0.4)\n",
        "    else:\n",
        "        # Only a curve/line on the contour (surface = separator)\n",
        "        cs = plt.contourf(xx, yy, Z, levels=[-1,0,1], cmap=plt.cm.coolwarm_r, alpha=0.4)\n",
        "    # Plot scatter plot\n",
        "    cmap = {1: 'blue', 0: 'red'}\n",
        "    colors = [cmap[c] for c in df[target_col]]\n",
        "    plt.scatter(data[col1], data[col2], color=colors)\n",
        "    # Build legend\n",
        "    plt.legend(handles=[mpatches.Patch(color=cmap[k], label=k) for k in cmap], loc=\"best\", title=\"Target\", frameon=True)\n",
        "    plt.show()\n",
        "\n",
        "#tree_depth=maxi_depth\n",
        "tree_depth=1\n",
        "viz_model = DecisionTreeClassifier(max_depth=tree_depth, criterion=\"entropy\")\n",
        "Decision_Surface(df, \"fare\", \"age\", df.survived, viz_model)\n",
        "visualize_tree(viz_model, [\"fare\",\"age\"], [\"dies\", \"survives\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-upQD5xbf_F"
      },
      "source": [
        "How good is our model? Let's compute accuracy--the percent of times where we correctly identified whether a passenger survives.  \n",
        "\n",
        "(*NOTE!  This confounds the model and the decision logic, but we need a little bit more to get to that.  See below.*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmSwBGe4bf_H"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "print ( \"Accuracy = %.3f\" % (metrics.accuracy_score(our_decision_tree.predict(df[predictor_cols]), df[\"survived\"])) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX8A7GQkbf_K"
      },
      "source": [
        "What are some other ways we could classify the data? The other bread-and-butter machine learning method is equation fitting, the most common instance being linear modeling; let's take a look to see how linear modeling partitions the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK55uf0dbf_K"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "our_lin_model = linear_model.LogisticRegression()\n",
        "Decision_Surface(df, \"fare\", \"age\", df.survived, our_lin_model, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMJ4JpeCbf_K"
      },
      "source": [
        "And this is the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vujNj4l3bf_L"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "our_lin_model.fit(df[predictor_cols], df.survived)\n",
        "print (\"Accuracy = %.3f\" % (metrics.accuracy_score(our_lin_model.predict(df[predictor_cols]), df.survived)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXhUEP6sUPKG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}