{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/connordouglas10/DS-Tech-2024spring/blob/main/Module6_Text_NaiveBayes/mining_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7pPYtSaoaDhB"},"outputs":[],"source":["#If opening in colab run this cell\n","!git clone https://github.com/connordouglas10/DS-Tech-2024spring.git\n","%cd DS-Tech-2024spring/Module6_Text_NaiveBayes"]},{"cell_type":"markdown","metadata":{"id":"jOGCfHdvaDhY"},"source":["# Dealing with text (and another ML algorithm: Naive Bayes)\n","\n","\n","Spring 2024 - Instructor: Foster Provost\n","\n","Teaching Assistant: Connor Douglas\n","***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XoTP4cq2aDhb"},"outputs":[],"source":["# Import the libraries we will be using\n","import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","import matplotlib.pylab as plt\n","%matplotlib inline\n","\n","# We will want to keep track of some different roc curves, lets do that here\n","tprs = []\n","fprs = []\n","roc_labels = []\n","aucs = []"]},{"cell_type":"markdown","metadata":{"id":"J8tQAY5YaDhm"},"source":["## Document classification and customer satisfaction"]},{"cell_type":"markdown","metadata":{"id":"0HyNJx_saDhq"},"source":["Our problem setting: You've been hired by Trans American Airlines (TAA) as a business analytics professional. One of the top priorities of TAA is  customer service. For TAA, it is of utmost importance to identify whenever customers are unhappy with the way employees have treated them. You've been hired to analyze twitter data in order to detect whenever a customer has complaints about flight attendants. Tweets suspected to be related to flight attendant complaints should be forwarded directly to the customer service department in order to track the issue and take corrective actions.  \n","\n","Let's start by loading the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6an9WIR1aDht"},"outputs":[],"source":["data_path = 'data/tweets.csv'\n","df = pd.read_csv(data_path)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"qMugFtwxaDh1"},"source":["Let's take a look at what do people complain about in Twitter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlvVuDRXf7Ai"},"outputs":[],"source":["df.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMy-_VvOaDh3"},"outputs":[],"source":["df.negativereason.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"GdFVqVr1aDiA"},"source":["We will define our target variable based on \"Flight Attendant Complaints\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qxv3vGbLaDiD"},"outputs":[],"source":["# We'll call our target variable \"is_fa_complaint\" and keep only the text as a \"feature\" (really, the text is the field from which we will engineer features)\n","df[\"is_fa_complaint\"] = (df.negativereason == \"Flight Attendant Complaints\").astype(int)\n","df = df[[\"is_fa_complaint\", \"text\"]]\n","df.shape"]},{"cell_type":"markdown","metadata":{"id":"PXKTkHj3aDiN"},"source":["Let's take a look at the percentage of tweets related to complaints about flight attendants, aka the base rate:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8UILyBmaDiP"},"outputs":[],"source":["df['is_fa_complaint'].mean()"]},{"cell_type":"markdown","metadata":{"id":"HJVqmlN3aDiX"},"source":["Here are some examples of the tweets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfmX69IwaDiZ"},"outputs":[],"source":["print(\"Flagged as FA complaints:\")\n","print(df[df.is_fa_complaint == 1].text.values[0:5])\n","\n","print(\"\\nNot flagged as FA complaint:\")\n","print(df[df.is_fa_complaint == 0].text.values[0:5])"]},{"cell_type":"markdown","metadata":{"id":"g-ehwyKkaDig"},"source":["Since we are going to do some predictive modeling, we should split our data into a training and a test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FhDqSSwEaDii"},"outputs":[],"source":["X = df['text']\n","Y = df['is_fa_complaint']\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"S9EaYDXRaDit"},"source":["### Text to features\n","How can we turn the large amount of text for each record into useful features?\n","\n","\n","#### Binary representation\n","One way is to create a matrix that uses each word as a feature and keeps track of whether or not a word appears in a document/record. You can do this in sklearn with a `CountVectorizer()` and setting `binary` to `true`. The process is very similar to how you fit a model: you will fit a `CounterVectorizer()`. This will figure out what words exist in your data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrjKfevpaDix"},"outputs":[],"source":["binary_vectorizer = CountVectorizer(binary=True)\n","binary_vectorizer.fit(X_train)"]},{"cell_type":"markdown","metadata":{"id":"qFZwIwwWaDi3"},"source":["Let's look at the vocabulary the `CountVectorizer()` learned."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CcpjAukaDi5"},"outputs":[],"source":["vocabulary_list = list(zip( binary_vectorizer.vocabulary_.keys(), binary_vectorizer.vocabulary_.values()) )\n","\n","vocabulary_list[0:10]"]},{"cell_type":"markdown","metadata":{"id":"5QVLEmreaDjB"},"source":["Now that we know what words are in the data, we can transform our text into a clean matrix. Simply .transform() the raw data using our fitted CountVectorizer(). You will do this for the training and test data. What do you think happens if there are new words in the test data that were not seen in the training data?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZdHAgoHaDjE"},"outputs":[],"source":["X_train_binary = binary_vectorizer.transform(X_train)\n","X_test_binary = binary_vectorizer.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNs8Hcel0x5t"},"outputs":[],"source":["X_train_binary.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x2M7l7ERgRpP"},"outputs":[],"source":["X_test_binary.shape"]},{"cell_type":"markdown","metadata":{"id":"IhsFILAhaDjL"},"source":["We can take a look at our new `X_test_binary`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrFT2qgmaDjN"},"outputs":[],"source":["X_test_binary"]},{"cell_type":"markdown","metadata":{"id":"Ia3glHapaDjU"},"source":["Sparse matrix? Where is our data?\n","\n","If you look at the output above, you will see that it is being stored in a *sparse* matrix (as opposed to the typical dense matrix) that is 3k rows long and 13k columns. The rows here are records in the original data and the columns are words. Given the shape, this means there are 39m cells that should have values. However, from the above, we can see that only 46k cells (~0.12%) of the cells have values! Why is this?\n","\n","To save space, sklearn uses a sparse matrix. This means that only values that are not zero are stored. This saves a ton of space! This also means that visualizing the data is a little trickier. Let's look at a very small chunk."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CA_LkpIKaDjW"},"outputs":[],"source":["# Recall that 13183 is the index for \"you\"\n","X_test_binary[0:20, 13180:13200].todense()"]},{"cell_type":"markdown","metadata":{"id":"gRRQtvk1aDjc"},"source":["#### Applying a model\n","Now that we have a ton of features (one for every word!) let's try using a logistic regression model to predict which tweets are about flight attendant complaints."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOm9gaMqaDje"},"outputs":[],"source":["def get_model_roc(models, Xs_test, names, Y_test):\n","    plt.rcParams['figure.dpi'] = 100\n","    for i in range(len(models)):\n","        model = models[i]\n","        X_test = Xs_test[i]\n","        name = names[i]\n","        probs = model.predict_proba(X_test)[:,1]\n","        fpr, tpr, thresholds = metrics.roc_curve(Y_test, probs)\n","        plt.plot(fpr, tpr, label=name)\n","        plt.plot([0, 1], [0, 1], linestyle='dashed', color='black')\n","        plt.xlabel(\"False Positive Rate\")\n","        plt.ylabel(\"True Positive Rate\")\n","        plt.title(\"ROC Curve\")\n","        print (\"AUC for {0} = {1:.3f}\".format(name, metrics.roc_auc_score(Y_test, probs)))\n","    plt.legend()\n","    plt.show()\n","\n","\n","model_binary = LogisticRegression(solver='liblinear')\n","model_binary.fit(X_train_binary, Y_train)\n","get_model_roc([model_binary], [X_test_binary], ['binary'], Y_test)\n","#Note that if you were doing this for real, you'd want to make sure you are regularizing well!"]},{"cell_type":"markdown","metadata":{"id":"qigohEb7aDjm"},"source":["#### Counts instead of binary\n","Instead of using a 0 or 1 to represent the occurence of a word, we can use the actual counts. We do this the same way as before, but now we leave `binary` set to `false` (the default value)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvaoF_F0aDjo"},"outputs":[],"source":["# Fit a counter\n","count_vectorizer = CountVectorizer()\n","count_vectorizer.fit(X_train)\n","\n","# Transform to counts\n","X_train_counts = count_vectorizer.transform(X_train)\n","X_test_counts = count_vectorizer.transform(X_test)\n","\n","# Model\n","model_counts = LogisticRegression(solver='liblinear')\n","model_counts.fit(X_train_counts, Y_train)\n","\n","get_model_roc([model_binary, model_counts], [X_test_binary, X_test_counts], ['binary', 'counts'], Y_test)"]},{"cell_type":"markdown","metadata":{"id":"bFOYPcXtaDju"},"source":["#### Tf-idf\n","Another popular technique when dealing with text is to use the term frequency - inverse document frequency (tf-idf) measure instead of just counts as the feature values (see the book)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Clx7EIPmaDjw"},"outputs":[],"source":["# Fit vectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf_vectorizer.fit(X_train)\n","\n","# Transform to tfidf\n","X_train_tfidf = tfidf_vectorizer.transform(X_train)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n","\n","# Model\n","model_tfidf = LogisticRegression(solver='liblinear')\n","model_tfidf.fit(X_train_tfidf, Y_train)\n","\n","get_model_roc([model_binary, model_tfidf], [X_test_binary, X_test_tfidf], ['binary', 'tf-idf'], Y_test)"]},{"cell_type":"markdown","metadata":{"id":"0Ay1-lPBaDj4"},"source":["The `CountVectorizer()` and `TfidfVectorizer()` functions have many options. You can restrict the words you would like in the vocabulary. You can add n-grams. You can use stop word lists. Which options you should use generally depends on the specific data you are dealing with."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XuKviO7-aDj5"},"outputs":[],"source":["# Try stop words and ngrams up to n=2\n","ngram_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n","ngram_vectorizer.fit(X_train)\n","\n","# Transform with the vectorizer\n","X_train_ngram = ngram_vectorizer.transform(X_train)\n","X_test_ngram = ngram_vectorizer.transform(X_test)\n","\n","# Fit the model\n","model_ngram = LogisticRegression(solver='liblinear')\n","model_ngram.fit(X_train_ngram, Y_train)\n","\n","get_model_roc([model_binary, model_ngram], [X_test_binary, X_test_ngram], ['binary', '2-ngram'], Y_test)"]},{"cell_type":"markdown","metadata":{"id":"0qopawLeaDj_"},"source":["### Modeling with another technique: Naive Bayes\n","\n","So far we have been exposed to tree-structured models and logistic regression in class, as well as non-linear methods based on these (like RandomForests and LR with polynomial features). Now, it's time for another popular modeling technique for supervised learning (especially in text classification): Naive Bayes (NB). In particular, we are using a Bernoulli Naive Bayes (BNB) for our binary classification task. (Bernoulli NB is the model described in the book; there are other versions of NB -- see below.)\n","\n","As described in your text, the Naive Bayes model is a **probabilistic approach which assumes conditional independence between features** (in this case, each word/ngram is a feature, the conditioning is on the true class). In other words, Naive Bayes models the probabilities of the presence of each _word_, given that we have a flight attendant complaint, and given that we do not have a flight attendant complaint.  Then it combines them using Bayes Theorem (again, as described in the book) to give an estimated probability of the class to an instance.\n","\n","Using this model in sklearn works just the same as the others we've seen ([More details here..](http://scikit-learn.org/stable/modules/naive_bayes.html))\n","\n","- Choose the model\n","- Fit the model (Train)\n","- Predict with the model (Train or Test or Use data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wPt2PqXaDkC"},"outputs":[],"source":["from sklearn.naive_bayes import BernoulliNB\n","\n","model_nb = BernoulliNB()\n","model_nb.fit(X_train_binary, Y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bo0O0Tm3aDkJ"},"outputs":[],"source":["get_model_roc([model_binary, model_nb], [X_test_binary, X_test_binary], ['binary', 'naive-bayes'], Y_test)"]},{"cell_type":"markdown","metadata":{"id":"wSZd088zaDkH"},"source":["The past few weeks we have seen that many of the models we are using have different complexity control hyperparameters that can be tweaked. In naive Bayes, the hyperparameter that is typically tuned is the Laplace smoothing value **`alpha`**.  You can try to see whether tuning alpha helps improve on the results above.\n","\n","Also, there are other versions of naive Bayes:\n","\n","1. **Multinomial naive Bayes (MNB):** This model handles count features and not just binary features. Sometimes MNB is used with binary presence/absence variables anyway (like word presence), even though that violates the model assumptions, because in practice it still works well.\n","2. **Gaussian Naive Bayes (GNB):** This model considers likelihood of the features as Gaussian--and thus we can use it for continuous features.  Sometimes GNB and Bernoulli NB are combined when one has features of mixed types.  \n","\n","You can try out all of these alternatives!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L29OXz_3dHft"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1jpJ6MgFbWNBreOtgrDX6PrjFUoJqJkJv","timestamp":1709678960560}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
