{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/connordouglas10/DS-Tech-2024spring/blob/main/Module3_Fitting_CrossVal/Fitting_and_overfitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2126,"status":"ok","timestamp":1708022444000,"user":{"displayName":"Connor Douglas","userId":"17662126372776058519"},"user_tz":300},"id":"ycRPJTieqv5n","outputId":"d3fceea5-b291-4b2c-acac-60d386ade961"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'DS-Tech-2024spring-ADMIN'...\n","remote: Enumerating objects: 133, done.\u001b[K\n","remote: Counting objects: 100% (133/133), done.\u001b[K\n","remote: Compressing objects: 100% (100/100), done.\u001b[K\n","remote: Total 133 (delta 38), reused 123 (delta 28), pack-reused 0\u001b[K\n","Receiving objects: 100% (133/133), 7.47 MiB | 12.48 MiB/s, done.\n","Resolving deltas: 100% (38/38), done.\n","/content/DS-Tech-2024spring-ADMIN/Module3_Fitting_CrossVal\n"]}],"source":["#If opening in colab run this cell\n","!git clone https://github.com/connordouglas10/DS-Tech-2024spring.git\n","%cd DS-Tech-2024spring/Module3_Fitting_CrossVal/"]},{"cell_type":"markdown","metadata":{"id":"Pd6-VDYOqv56"},"source":["# Fitting models and overfitting\n","\n","\n","2024 Spring - Instructors: Foster Provost and Connor Douglas\n","\n","Teaching Assistant: Connor Douglas\n","***"]},{"cell_type":"markdown","metadata":{"id":"EcNbGuqDqv5-"},"source":["## Packages"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2969,"status":"ok","timestamp":1708022450896,"user":{"displayName":"Connor Douglas","userId":"17662126372776058519"},"user_tz":300},"id":"h91yJ5bHqv6B"},"outputs":[],"source":["# Import the libraries we will be using\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","import math\n","import matplotlib.pylab as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","sns.set(style='ticks', palette='Set2')\n","\n","# some custom libraries!\n","import sys\n","sys.path.append(\"..\")\n","from ds_utils.decision_surface import *"]},{"cell_type":"markdown","metadata":{"id":"85BffofYqv6L"},"source":["Notice that we're importing library code that we've developed just for this class. In the future, new common code will continue to be added to the `ds_utils` folder."]},{"cell_type":"markdown","metadata":{"id":"YFHQz-n-qv6O"},"source":["## Motivational example\n","\n","To look more carefully at predictive modeling, imagine \"our data\" are some noisy observations from a nonlinear function. We're going to approximate that function by fitting a polynomial to the observations."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":464},"executionInfo":{"elapsed":1423,"status":"ok","timestamp":1708022459833,"user":{"displayName":"Connor Douglas","userId":"17662126372776058519"},"user_tz":300},"id":"zwI4UyFoqv6Q","outputId":"b9b65130-d8b4-4db3-c386-14d68764f56e"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#Create the data and show them\n","#In order that we can plot the data points and the function, we will just have one feature (x1)\n","num_samples = 50\n","# Set randomness so that we all get the same answer\n","np.random.seed(42)\n","\n","def true_function(X):\n","    return np.sin(1.5 * np.pi * X)\n","\n","def plot_example(X, Y, functions):\n","    # Get some X values to plot the functions\n","    X_test = pd.DataFrame(np.linspace(0, 1, 100), columns=['x1'])\n","    # Plot data and true function\n","    for key in functions:\n","        plt.plot(X_test, functions[key](X_test), label=key)\n","    plt.scatter(X, Y, edgecolor='b', s=20, label=\"Samples\")\n","    plt.xlabel(\"x\")\n","    plt.ylabel(\"y\")\n","    plt.xlim((0, 1))\n","    plt.ylim((-2, 2))\n","    plt.legend(loc=\"best\")\n","\n","# Add X in the range of [0, 1]\n","X = pd.DataFrame(np.sort(np.random.rand(num_samples)), columns=['x1'])\n","# Add some random noise to the observations\n","Y = true_function(X.x1) + np.random.randn(num_samples) * 0.5\n","# Plot stuff\n","#functions = {\"True function\": true_function}\n","functions = {}\n","plot_example(X, Y, functions)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"VIzxlZg4qv6e"},"source":["Let's assume that we don't know the true function.  We choose to model our noisy observations using linear regression.  (Compare with the fitting of models for binary target variables from last class.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NcempG6Iqv6g"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","\n","# Fit linear model\n","model = LinearRegression()\n","model.fit(X, Y)\n","# Evaluate model with mean squared error; just as an example!\n","mse = mean_squared_error(Y, model.predict(X))\n","r2 = r2_score(Y, model.predict(X))\n","# Plot results\n","functions[\"Model\"] = model.predict\n","plot_example(X, Y, functions)\n","#Note how you can customize your plots\n","#plt.title(\"Linear Model\\n MSE: %.2f\" % mse)\n","#plt.title(\"Linear Model\\n r2: %.2f\" % r2)\n","plt.title(\"Linear Model\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"piDG3R7owB0c"},"source":["Does the linear regression fit our data well?"]},{"cell_type":"markdown","metadata":{"id":"eGTGXfvCqv6q"},"source":[".\n","\n","\n","Rather than trying a linear regression, let's make our functional form more complex.  We will fit non-linear regressions, specifically polynomial regressions. How do different degree polynomials fit the data? Recall that a polynomial on a single variable looks like:\n","\n","$$ a_1 + a_2 x + a_3 x^2 + ... $$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5d0A_-rpqv6v"},"outputs":[],"source":["#Create function to fit different degree polynomials\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","def fit_polynomial(X, Y, degree):\n","    # create different powers of X\n","    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n","    linear_regression = LinearRegression()\n","    pipeline = Pipeline([(\"polynomial_features\", polynomial_features), (\"linear_regression\", linear_regression)])\n","    pipeline.fit(X, Y)\n","    return pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zucPGlosqv64"},"outputs":[],"source":["#Now let's see what we have learned\n","\n","def plot_poly(X, Y, degree):\n","    # Fit polynomial model\n","    model = fit_polynomial(X, Y, degree)\n","    # Evaluate model\n","    mse = mean_squared_error(Y, model.predict(X))\n","    # Plot results\n","    functions[\"Model\"] = model.predict\n","    plt.title(\"Degree %d\\n MSE: %.2f\" % (degree, mse))\n","    plot_example(X, Y, functions)\n","\n","plot_poly(X, Y, degree=2)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ESZMHf9nqv7A"},"source":["This seems to fit our data better than the purely linear model. What if we use polynomials with higher degrees?\n","\n","(Remember -- the ML doesn't see the true function curve!  And neither do we when we are evaluating the model/predictions.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Re2xvAzqv7B","scrolled":true},"outputs":[],"source":["plt.figure(figsize=(14, 5))\n","# degrees of the polynomial\n","degrees = [1, 2, 3]\n","for i in range(len(degrees)):\n","    ax = plt.subplot(1, len(degrees), i + 1)\n","    plt.setp(ax, xticks=(), yticks=())\n","    plot_poly(X, Y, degrees[i])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"VkNSSOf0qv7K"},"source":["What do you see there as the effect of allowing more complexity in the modeling process? Take a look at what happens when we use a **regression tree** on data generated from the *true function*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XwBeb42tqv7L"},"outputs":[],"source":["from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","Expected_Y = true_function(X.x1)\n","plt.figure(figsize=(14, 5))\n","# Fit Regression Trees\n","depths = [1]\n","#depths = [1,2]\n","for i, depth in enumerate(depths):\n","    ax = plt.subplot(1, len(depths), i + 1)\n","    #plt.setp(ax, xticks=(), yticks=())\n","    plt.setp(ax, yticks=())\n","    model = DecisionTreeRegressor(max_depth=depth)\n","    model.fit(X, Expected_Y)\n","    functions = {\"True function\": true_function, \"Tree (depth {})\".format(depth): model.predict}\n","    plot_example(X, Expected_Y, functions)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"8bm3k1wDqv7U"},"source":["## Predicting wine quality\n","\n","_\"All wines should be tasted; some should only be sipped, but with others, drink the whole bottle.\"_ - Paulo Coelho, Brida\n","\n","We will use a data set related to the red variant of the Portuguese \"Vinho Verde\" wine. We will predict the \"sensory\" output based on physicochemical inputs.  (Here there is no data about grape types, wine brand, wine selling price, etc.). Our goal is to use machine learning to detect above-average wines (perhaps to send these wines later to professional tasters?).\n","\n","Let's start by loading the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8r7X9ZBzqv7W"},"outputs":[],"source":["url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n","wine_df = pd.read_csv(url, delimiter=\";\").dropna()\n","wine_df.head(15)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwOwfjKXxkjO"},"outputs":[],"source":["wine_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EAw32kMmxQbE"},"outputs":[],"source":["# Now, let's change the label to reflect our decision problem, namely, to identify above-average wines.\n","avg_quality = wine_df.quality.mean()\n","wine_df[\"is_good\"] = wine_df.quality > avg_quality\n","#Note above the \"Pandas\" way of doing things: process all the instances simultaneously\n","#   computing the mean in one swoop; assigning the new column to the instances all at once.\n","\n","#Now we will get rid of the old feature, quality.\n","#  Ask yourself: what would have happened if had used quality in predicting the new target?\n","#    (Hint: leakage!  But make sure you understand exactly why.)\n","wine_df = wine_df.drop(\"quality\", axis=\"columns\")\n","# Replace white spaces with underscores in column names\n","wine_df.columns = [c.replace(' ', '_') for c in wine_df.columns]\n","wine_df.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6xz0dQvyMR8"},"outputs":[],"source":["#Now let's set ourselves up for predictive modeling\n","# Get column names and predictor columns\n","column_names = wine_df.columns\n","predictor_columns = column_names[:-1]"]},{"cell_type":"markdown","metadata":{"id":"PIMnEnSDqv7e"},"source":["Let's see if any of the features seem to be very predictive by themselves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Umm_zW_hqv7h"},"outputs":[],"source":["rows = 4\n","cols = 3\n","fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(5*cols, 6*rows))\n","axs = axs.flatten()\n","for i in range(len(predictor_columns)):\n","        wine_df.boxplot(predictor_columns[i], by=\"is_good\", grid=False, ax=axs[i], sym='k.')\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"iMrqDpWAqv7o"},"source":["There's no single feature that can separate the data perfectly. Alcohol and total sulfur dioxide look somewhat predictive though."]},{"cell_type":"markdown","metadata":{"id":"DS6saENcqv7r"},"source":["## Tree-structured models\n","Let's now re-explore the modeling technique we introduced last class -- tree-structured models.  And in particular, classification trees, since our target is to predict (the probability of) whether the wine is good or not -- binary classification (class probability estimation)."]},{"cell_type":"markdown","metadata":{"id":"ZoPxBZNCqv7t"},"source":["For illustration, we will increase the complexity of the tree using the maximum depth allowed. (Note that using max_depth is for illustration -- I recommend using the minimum number of instances at a leaf or at a split in practice; we can talk about that.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPKGM9WPqv7v"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","X = wine_df[predictor_columns]\n","Y = wine_df.is_good\n","\n","def training_accy(X, y, model):  #to add accuracy metric to plots - as an example!\n","    y_hat = model.fit(X, y).predict(X)\n","    return accuracy_score(y, [1 if ty >= 0.5 else 0 for ty in y_hat])\n","\n","def plot_trees(X, Y, col1, col2, depths, show_probs=False, show_acc=False):\n","    ncol = 3\n","    nrows = int(np.ceil(len(depths) / ncol))\n","    plt.figure(figsize=[15, 7*nrows])\n","\n","    for i in range(len(depths)):\n","        depth = depths[i]\n","        # Plot\n","        plt.subplot(nrows, ncol, 1+i)\n","        model = DecisionTreeClassifier(max_depth=depth, criterion=\"entropy\")\n","        Decision_Surface(X, col1, col2, Y, model, sample=0.1, gridsize=100,probabilities=show_probs)\n","        model.fit(X,Y)\n","        acc = training_accy(X,Y,model)\n","        if show_acc:\n","          plt.title(f\"Decision Tree Classifier (max depth={depth}, acc = {acc:.2f})\")\n","        else:\n","          plt.title(f\"Decision Tree Classifier (max depth={depth})\")\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_trees(X, Y, \"alcohol\", \"total_sulfur_dioxide\", depths=[1,2,3], show_probs=False, show_acc=False)"]},{"cell_type":"markdown","metadata":{"id":"kcahDBUAqv72"},"source":["### Trees can represent any function of the input to arbitrary precision\n","\n","If you experiment with the tree depth, you will see that you can fit the data better and better. Deeper trees chop the instance space into smaller and smaller pieces.  Check it out below with the `depths` variable. (Will this finer and finer segmentation go on forever?)\n","\n","**Thought exercise**: Tree learning can fit the data up to the theoretical limit of accuracy.  Is this 100%  If not, why not?\n","\n","**Extra:** Can you visualize the actual tree-structured model?  Hint: there's a function to do it in last week's notebook.  [Caveat: Visualizing huge trees isn't so effective.]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8c-aBFpqv73","scrolled":false},"outputs":[],"source":["plot_trees(X, Y, \"alcohol\", \"total_sulfur_dioxide\", depths=[1,2,3,4,5,6,10,20,30])"]},{"cell_type":"markdown","metadata":{"id":"2dVwXLDfqv8B"},"source":["## Linear discriminant models\n","\n","Chapter 4 introduces linear models.  We've built one already -- a linear regression.  Let's try building a linear model for this prediction problem.\n","\n","Looking at the data (see scatterplots above), can you estimate by eye where a good **linear discriminant** would be?  (What's a linear discriminant again?)\n","\n","If you remember, *linear regression* looks like this:\n","\n","$$ y = b + a_1 x_1 + a_2 x_2 + a_3 x_3 + ... $$\n","\n","If you are estimating the *probability* of one of two different classes, traditional linear regression won't work as well as some people hope. Probabilities need to be bounded between zero and one. To solve this problem, one of the most common machine learning tools is **logistic regression**.  Chapter 4 describes it. You can also find logistic regression modeling in the sklearn package.\n","\n","Let's plot both linear regression and logistic regression together to compare them.."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xyYHIrjmqv8D"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","\n","def plot_linear(X, Y, col, model_type, ymin=-0.1, ymax=1.1, sample=1):\n","    if model_type == \"Linear Regression\":\n","        model = LinearRegression()\n","        predict_fn = model.predict\n","    else:\n","        model = LogisticRegression()\n","        predict_fn = lambda obs: model.predict_proba(obs)[:, 1]\n","    title = model_type + \" Regression\"\n","    # Fit model\n","    col_min = X[col].min()\n","    col_max = X[col].max()\n","    col_df = pd.DataFrame(X[col], columns=[col])\n","    model.fit(col_df, Y)\n","    # Evaluate predictions\n","    Y_pred = predict_fn(col_df)\n","    mse = mean_squared_error(Y, Y_pred)\n","    # Plot prediciton line\n","    col_line = pd.DataFrame(np.linspace(col_min, col_max, 100), columns=[col])\n","    plt.plot(col_line, predict_fn(col_line))\n","    # Plot sample\n","    indices = np.random.permutation(range(len(Y)))[:int(sample*len(Y))].tolist()\n","    plt.scatter(col_df[col][indices], Y[indices], edgecolor='b')\n","    plt.xlabel(col)\n","    plt.ylabel(\"Good?\")\n","    plt.xlim((col_min, col_max))\n","    plt.ylim((ymin, ymax))\n","    plt.title(\"%s, MSE %0.3f\" % (title, mse))\n","\n","def linear_predict(model, X):\n","    return model.predict(X)\n","\n","def logistic_predict(model, X):\n","    return model.predict_proba(X)[:, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9oUfMnkqv8I"},"outputs":[],"source":["plt.figure(figsize=[15,7])\n","\n","plt.subplot(1, 2, 1)\n","plot_linear(X, Y, \"alcohol\", \"Linear Regression\")\n","\n","#plt.subplot(1,2,2)\n","#plot_linear(X, Y, \"alcohol\", \"Logistic Regression\")"]},{"cell_type":"markdown","metadata":{"id":"yLWnU0I1qv8P"},"source":["And, of course, we can look at the decision surface produced by logistic regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DuzQXbc6qv8W"},"outputs":[],"source":["plt.figure(figsize=[7,7])\n","\n","#plt.subplot(1, 2, 1)\n","#Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, LinearRegression(), sample=0.1, probabilities=False)\n","#lin_accy = training_accy(X[[\"alcohol\", \"total_sulfur_dioxide\"]], Y, LinearRegression())\n","#plt.title(\"Linear Regression, Accy: %0.3f\" % lin_accy)\n","\n","plt.subplot(1, 1, 1)\n","Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, LogisticRegression(), sample=0.1, probabilities=False)\n","lr_accy = training_accy(X[[\"alcohol\", \"total_sulfur_dioxide\"]], Y, LogisticRegression())\n","plt.title(\"Logistic Regression, Accy: %0.3f\" % lr_accy)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"CP0g7rSVqv8f"},"source":["### Estimating Probabilities\n","\n","\n","For many business problems, we don't need just to estimate the categorical target variable, but we want to estimate the probability that a particular value will be taken. Just about every classification model can also tell you the estimated probability of class membership.\n","\n","Intuitively, how would you generate probabilities from a classification tree? From a linear discriminant?\n","\n","Let's look at the probabilities estimated by these models. As shown below, you can visualize the probabilities both for the linear model and the tree-structured model. Note that the native `LinearRegression` class in sklearn doesn't have probability estimation capability (Why do you think?). We can only perform this operation with logistic regression."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfXeBr6Fqv8g","scrolled":true},"outputs":[],"source":["plt.figure(figsize=[15,7])\n","\n","plt.subplot(1, 2, 1)\n","depth=5\n","model = DecisionTreeClassifier(max_depth=depth, criterion=\"entropy\")\n","Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, model, sample=0.1, probabilities=True)\n","plt.title(\"Decision tree with depth \" + str(depth))\n","\n","plt.subplot(1, 2, 2)\n","model = LogisticRegression()\n","Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, model, sample=0.1, probabilities=True)\n","plt.title(\"Logistic regression\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"iR5oFC8Qqv8m"},"source":["Let's revisit the deeper and deeper trees from above, but this time visualizing the probabilities.  \n","\n","(Do the probabilities for the last trees look odd? )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"maXYMjomqv8o"},"outputs":[],"source":["plot_trees(X, Y, \"alcohol\", \"total_sulfur_dioxide\", depths=[1,2,3,4,5,6,10,20,30], show_probs=True)"]},{"cell_type":"markdown","metadata":{"id":"josPF_n-qv8u"},"source":["### Non-linear equation-based models\n","\n","Tree-structured models are non-linear, and can fit the data very well. It seems like a linear model possibly cannot. Can we use the mechanism of fitting linear models to generate non-linear boundaries with logistic regression?\n","\n","Yes! We did this already for numeric regression.  Here we also add non-linear features, such as  $ x^2 $  or  $ x^3 $ for any feature $ x $. We can even include a full set of polynomial feature interactions: given input features $x_1$ and $x_2$, we can, for instance,  build models and prediction on $x_1 + x_2 + x_1^2 + x_2^2 + x_1x_2$.\n","\n","This is one of the most common ways of introducing non-linearity into numeric function modeling: use a linear function learner, but introduce non-linear features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ctLa3SHsqv8v"},"outputs":[],"source":["def polynomial_model(model=LogisticRegression(), degree=1):\n","    polynomial_features = PolynomialFeatures(degree=degree, include_bias=True)\n","    pipeline = Pipeline([(\"polynomial_features\", polynomial_features), (\"model\", model)])\n","    return pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kljC52U4qv8z"},"outputs":[],"source":["plt.figure(figsize=[15,7])\n","from sklearn.linear_model import LogisticRegression\n","degrees = [1,2,3]\n","for i in range(len(degrees)):\n","    model = polynomial_model(LogisticRegression(solver='liblinear',max_iter=1000), degrees[i])\n","    plt.subplot(1, len(degrees), i+1)\n","    Decision_Surface(X, \"alcohol\", \"total_sulfur_dioxide\", Y, model, probabilities=True, sample=0.1)\n","    accy = training_accy(X, Y, model)\n","    plt.title(\"Degree %d, Accy: %0.3f\" % (degrees[i], accy))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"sQwiCOHWqv84"},"source":["Which model is better in this case?? Look at the **accuracy** of each one. Accuracy is simply the count of correct decisions divided by the total number of decisions.  You should ask: wait a minute: what is the decision logic?  Here we are just choosing the class estimated to be the most probable.  Also, note that here we are computing the accuracy of the model when it makes predictions on the training set, examples the modeling process \"already has seen the answers for\".\n","\n","[From sklearn documentation on sklearn.metrics.accuracy_score: \"In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\"  [More about the accuracy measure..](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)]"]},{"cell_type":"markdown","metadata":{"id":"0UDGCyOgqv86"},"source":["## Generalization\n","\n","Our evaluation above actually was not what we really want.\n","\n","What we want are models that **generalize** to data that were not used to build them! In other words, we want this model to be able to predict the target for new data instances! Do we know how well our models generalize? Why is this important?\n","\n","Let's apply this concept to our data. Now, before we fit out models, we set aside some data to be used later for testing ('holdout data').  This allows us to assess whether the model simply fit the training dataset well, or whether it truly found generalizable regularities.\n","\n","Let's use sklearn to set aside some randomly selected holdout data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5jqCR02Hqv88"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Set randomness so that we all get the same answer\n","np.random.seed(42)\n","#np.random.seed(43)\n","shuffled_df = wine_df.sample(frac=1)\n","X = shuffled_df[predictor_columns]\n","Y = shuffled_df.is_good\n","# Split the data into train and test pieces for both X and Y\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n","\n","model = DecisionTreeClassifier(max_depth=10)\n","model.fit(X_train, Y_train)\n","\n","print ( \"Accuracy on training = %.4f\" % accuracy_score(model.predict(X_train), Y_train) )\n","print ( \"Accuracy on test = %.4f\" % accuracy_score(model.predict(X_test), Y_test) )"]},{"cell_type":"markdown","metadata":{"id":"QJUCsCrKqv9D"},"source":["Accuracy on the training set is better than on the test set! Why is this? What can we do to make things better? What happens if our tree gets even deeper?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olhgtK9Sqv9F"},"outputs":[],"source":["def plot_fitting_curve(datasets, maxdepth=15):\n","    # Intialize accuracies\n","    accuracies = {}\n","    for key in datasets:\n","        accuracies[key] = []\n","    # Initialize depths\n","    depths = range(1, maxdepth+1)\n","    # Fit model for each specific depth\n","    for md in depths:\n","        model = DecisionTreeClassifier(max_depth=md, random_state=42)\n","        # Record accuracies\n","        for key in datasets:\n","            X = datasets[key]['X']\n","            Y = datasets[key]['Y']\n","            if key == \"X-Val\":\n","                accuracies[key].append(cross_val_score(model, X, Y, scoring=\"accuracy\", cv=5).mean())\n","            else:\n","                model.fit(datasets['Train']['X'], datasets['Train']['Y'])\n","                accuracies[key].append(accuracy_score(model.predict(X), Y))\n","    # Plot each curve\n","    plt.figure(figsize=[10,7])\n","    for key in datasets:\n","        plt.plot(depths, accuracies[key], label=key)\n","    # Plot details\n","    plt.title(\"Performance on train and test data\")\n","    plt.xlabel(\"Max depth\")\n","    plt.ylabel(\"Accuracy\")\n","    # Find minimum accuracy in all runs\n","    min_acc = np.array(list(accuracies.values())).min()\n","    plt.ylim([min_acc, 1.0])\n","    plt.xlim([1, maxdepth])\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","\n","datasets = {\"Train\": {\"X\": X_train, \"Y\": Y_train}, \"Test\": {\"X\": X_test, \"Y\": Y_test}}\n","plot_fitting_curve(datasets)"]},{"cell_type":"markdown","metadata":{"id":"Q1Frb8goqv9J"},"source":["## Cross validation\n","\n","Above, we made a single train/test split. We set aside 20% of our data and *never* used it for training. We also never used the 80% of the data set aside for training to test generalizability.  Although this is far better than testing on the training data, which does not measure generalization performance at all, **there are two potential problems with the simple holdout approach**.\n","\n","1) Perhaps the random split was particularly bad (or good).  Do we have any confidence in our accuracy estimate?\n","\n","2) We are using only 20% of the data for testing.  Could we possibly use the data more fully for testing?\n","\n","3) Often we want to know something about the distribution of our evaluation metrics. A simple train/test split only allows a single \"point estimate\"\n","\n","Instead of only making the split once, let's use **cross-validation** -- every record will contribute to testing as well as to training.\n","\n","\n","<img src=\"https://github.com/pearl-yu/foster_2022fall/blob/2022-master/Module3_Fitting_CrossVal/images/cross.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-00kwLPBqv9L"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","\n","model = DecisionTreeClassifier(max_depth=10)\n","scores = cross_val_score(model, X, Y, scoring=\"accuracy\", cv=5)\n","\n","print (\"Cross Validated Accuracy: %0.3f +/- %0.3f\" % (scores.mean(), scores.std()))"]},{"cell_type":"markdown","metadata":{"id":"vEKvgUFOqv9Q"},"source":["We can add this cross-validated accuracy to our fitting plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xW0w-o2Tqv9S"},"outputs":[],"source":["datasets[\"X-Val\"] = {\"X\": X, \"Y\": Y}\n","plot_fitting_curve(datasets)"]},{"cell_type":"markdown","metadata":{"id":"HtZBSR__qv9X"},"source":["In this particular example, the performance on the test set does not drop as the trees get deeper.\n","\n","The book shows this:\n","\n","<img src=\"https://github.com/pearl-yu/foster_2022fall/blob/2022-master/Module3_Fitting_CrossVal/images/generalization.png?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"pngHUjZe_Tsm"},"source":["Note that cross-validation being \"better\" in the graph above is just an accident of the randomization.  The point isn't that CV gives better *performance*.  Using CV we do average over the different runs, so we generally reduce the variance.  See the example below, where a different random draw for the held-out test set give very different performance from the original (single) test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFdb3G0e-BdH"},"outputs":[],"source":["#make another test set for illustration:\n","X_train_2, X_test_2, Y_train_2, Y_test_2 = train_test_split(X_test, Y_test, test_size=0.5, random_state=49)\n","\n","\n","datasets[\"Alt. Test\"] = {\"X\": X_test_2, \"Y\": Y_test_2}\n","plot_fitting_curve(datasets)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UdosMEAW-fWF"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"wqYITcho97j3"},"source":["\n","So... take a look at the Homework example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7CM4Xwxqv9Z"},"outputs":[],"source":["# Load data\n","path = \"./data/data-hw1.csv\"\n","np.random.seed(42)\n","df = pd.read_csv(path)\n","# Shuffle data\n","# Get features and label\n","columns = [\"GRE Score\", \"TOEFL Score\", \"University Rating\", \"SOP\", \"LOR\", \"CGPA\", \"Research\"]\n","X = df[columns]\n","Y = df[\"Chance of Admit\"] > 0.5\n","# Split the data into train and test pieces for both X and Y\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n","# Test for overfitting\n","datasets = {\"Train\": {\"X\": X_train, \"Y\": Y_train},\n","            \"Test\": {\"X\": X_test, \"Y\": Y_test},\n","            \"X-Val\": {\"X\": X, \"Y\": Y}}\n","plot_fitting_curve(datasets, maxdepth=10)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1meYVLm0dG3fPKnT1a5FzwOyyaqnmJhTD","timestamp":1707747097880}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
